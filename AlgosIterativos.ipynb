{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0fc925e-bc76-4a15-a990-d7763d34b0b4",
   "metadata": {},
   "source": [
    "# Implementacion Pedagogica de Algoritmos \n",
    "A continuacion vemos la implementacion de los algoritmos vistos en clase. Estos son:\n",
    "* Steepest Descent\n",
    "* Newton\n",
    "* Cuasi-Newton\n",
    "\n",
    "\n",
    "## Parametros de los algoritmos iterativos\n",
    "* toleracia (abs_tol)\n",
    "$$\\frac{|x_{k+1}-x_k|}{|x_k|} < \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1436da0-7b7b-4fd4-91ee-15b9f1c9e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "416ea3ae-194f-4aee-a35f-f641fa1d3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(f, x_k, grad_k, p_k, imprimir=False, max_tries=100):\n",
    "    # Algoritmos de busqueda de longitud de paso\n",
    "    # Tambien llamado algoritmo de Backtracking\n",
    "\n",
    "    ## ENTRADAS\n",
    "    # f la funcion objetivo\n",
    "    # x_k el valor actual de la iteracion\n",
    "    # grad_k el valor del gradiente en x_k\n",
    "    # p_k la direccion de mejora\n",
    "    # parametros\n",
    "    \n",
    "    # longitud de paso default\n",
    "    alfa = 1.0\n",
    "    # factor de contraccion\n",
    "    rho = 0.9\n",
    "    # Condicion (1) de Wolfe\n",
    "    c_1 = 10e-4\n",
    "\n",
    "    Sentinela = 0\n",
    "    #print(f\"Wolfe condition terms: {f(x_k + alfa*p_k)}  {f(x_k) + c_1*alfa*grad_k.dot_product(p_k)}\")\n",
    "    while (f(x_k + alfa*p_k) > f(x_k) + c_1*alfa*grad_k.dot_product(p_k) and \n",
    "           Sentinela < max_tries):\n",
    "        if imprimir: #\n",
    "            print(f\"{alfa=}\")\n",
    "        alfa = rho*alfa\n",
    "        Sentinela += 1\n",
    "    if imprimir:\n",
    "        print(f\"{Sentinela=}\")\n",
    "    return alfa    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee87e50-b075-48c5-b2b5-3db96ad32977",
   "metadata": {},
   "source": [
    "### Funciones cuadraticas\n",
    "Tienen la forma:\n",
    "$$ f(x) = \\frac12 x^T Q x - b^T x$$\n",
    "Donde $Q$ es una matrix definida positiva,\n",
    "\n",
    "El gradiente es: $\\nabla f(x) = Qx - b$\n",
    "\n",
    "El hessiano: $\\nabla^2 f(x) = Q$\n",
    "\n",
    "La longitud de paso Optima para la iteracion $k$ es:\n",
    "$$\\alpha_k = \\frac{\\nabla f_k^T \\nabla f_k}{{\\nabla f_k^T Q \\nabla f_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f09738-b033-43fc-969a-79f899f0187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los Val. Propios de Q son [1.179678806250474?, 2.471162835631458?, 12.34915835811807?]\n",
      "El minimo de f esta en (-0.166666666666667, 0.166666666666667, 1.16666666666667)\n"
     ]
    }
   ],
   "source": [
    "# Generamos funciones cuadraticas de pruebas\n",
    "# generamos una matrix de numeros enteros de 3X3\n",
    "dim = 3\n",
    "M = random_matrix(ZZ, dim, dim)\n",
    "# M^TM siempre definida semipositiva\n",
    "Q = M.transpose()*M\n",
    "#Q = identity_matrix(3)\n",
    "print(f\"Los Val. Propios de Q son {Q.eigenvalues()}\")\n",
    "b = vector(range(dim))\n",
    "#b = vector([1,1,1])\n",
    "min_f = vector(RR, (Q.inverse()*b))\n",
    "print(f\"El minimo de f esta en {min_f}\")\n",
    "# funciones lambda = funciones anonimas (inline en C++)\n",
    "# np es numpy\n",
    "cuad = lambda x: 1/2*x*(Q*x) - b*x\n",
    "grad_cuad = lambda x: Q*x - b\n",
    "hess_cuad = lambda x: Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349728c-1f8e-4a09-aecf-97cdcd36c9b2",
   "metadata": {},
   "source": [
    "### La Funcion de Rosenbrock\n",
    "Es una funcion de prueba muy usada en optimizacion numerica, especialmente en el analisis de algoritmos iterativos https://en.wikipedia.org/wiki/Rosenbrock_function\n",
    "\n",
    "La version mas comun en dos dimensiones es:\n",
    "$$f(x,y) = (1 - x)^2 + 100(y-x^2)^2$$\n",
    "Tiene un minimo global en (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6f0c7d4-bd90-4479-93b8-0c0e08cb6a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\\(\\displaystyle \\left( x, y \\right) \\ {\\mapsto} \\ \\left(400 \\, {\\left(x^{2} - y\\right)} x + 2 \\, x - 2,\\,-200 \\, x^{2} + 200 \\, y\\right)\\)</html>"
      ],
      "text/latex": [
       "$\\displaystyle \\left( x, y \\right) \\ {\\mapsto} \\ \\left(400 \\, {\\left(x^{2} - y\\right)} x + 2 \\, x - 2,\\,-200 \\, x^{2} + 200 \\, y\\right)$"
      ],
      "text/plain": [
       "(x, y) |--> (400*(x^2 - y)*x + 2*x - 2, -200*x^2 + 200*y)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<html>\\(\\displaystyle \\left(\\begin{array}{rr}\n",
       "\\left( x, y \\right) \\ {\\mapsto} \\ 1200 \\, x^{2} - 400 \\, y + 2 & \\left( x, y \\right) \\ {\\mapsto} \\ -400 \\, x \\\\\n",
       "\\left( x, y \\right) \\ {\\mapsto} \\ -400 \\, x & \\left( x, y \\right) \\ {\\mapsto} \\ 200\n",
       "\\end{array}\\right)\\)</html>"
      ],
      "text/latex": [
       "$\\displaystyle \\left(\\begin{array}{rr}\n",
       "\\left( x, y \\right) \\ {\\mapsto} \\ 1200 \\, x^{2} - 400 \\, y + 2 & \\left( x, y \\right) \\ {\\mapsto} \\ -400 \\, x \\\\\n",
       "\\left( x, y \\right) \\ {\\mapsto} \\ -400 \\, x & \\left( x, y \\right) \\ {\\mapsto} \\ 200\n",
       "\\end{array}\\right)$"
      ],
      "text/plain": [
       "[(x, y) |--> 1200*x^2 - 400*y + 2               (x, y) |--> -400*x]\n",
       "[              (x, y) |--> -400*x                  (x, y) |--> 200]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# version simbolica\n",
    "var('x y')\n",
    "Rosen(x,y) = (1 - x)^2 + 100*(y - x^2)^2\n",
    "show(Rosen.gradient())\n",
    "show(Rosen.hessian())\n",
    "HessianoRosen = Rosen.hessian()(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66832682-ddf5-405c-b601-c5ee1c2f2347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399.999999999989"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod([N(_) for _ in HessianoRosen.eigenvalues()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20777f54-04c9-47e7-b4d3-be5440d808fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(0, 0)\n",
      "[ 802 -400]\n",
      "[-400  200]\n"
     ]
    }
   ],
   "source": [
    "def rosen(x):\n",
    "    ## funcion accesorio para en los algoritmos de optimizacion\n",
    "    return (1 - x[0])^2 + 100*(x[1] - x[0]^2)^2\n",
    "\n",
    "Grad_Rosen = Rosen.gradient()\n",
    "grad_rosen = lambda x: Grad_Rosen(x[0], x[1])\n",
    "\n",
    "Hess_Rosen = Rosen.hessian()\n",
    "hess_rosen = lambda x: Hess_Rosen(x[0], x[1])\n",
    "    \n",
    "print(rosen(vector([1,1])))\n",
    "print(grad_rosen(vector([1,1])))\n",
    "print(hess_rosen(vector([1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c9e1e-bdab-410d-b438-469a0347df77",
   "metadata": {},
   "source": [
    "## Steepest Descent\n",
    "Es algoritmo mas basico, solo usa el gradiente de la siguiente manera:\n",
    "$$p_k = -\\nabla f(x_k)$$\n",
    "y luego de escoger la longitud de paso $a_k$ (con algun algoritmo como backtracking) se encuentra el termino siguiente $x_{k+1} = x_k + \\alpha_k p_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37951f86-02c7-4012-bc20-2989bfae06a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.167', '0.167', '1.17'] con alpha=0.1501, contador=98, error_ajustado=0.00000094\n"
     ]
    }
   ],
   "source": [
    "# Implementamos el metodo del Steepest Descent\n",
    "# punto inicial para comenzar la iteracion\n",
    "x_0 = vector([2,2.5,4])\n",
    "# placeholder para x_k\n",
    "# el vector que va a llevar las iteraciones\n",
    "x_k = x_0\n",
    "contador = 0\n",
    "MAX_ITER = 1_000\n",
    "abs_tol = 1e-6\n",
    "error_ajustado = 1\n",
    "#pbar = tqdm(total = int(MAX_ITER + 1))\n",
    "while (contador < MAX_ITER and\n",
    "      error_ajustado > abs_tol):\n",
    "    grad_k = grad_cuad(x_k)\n",
    "    p_k = -grad_cuad(x_k)\n",
    "    alpha = backtrack(cuad, vector(x_k), vector(grad_k), vector(p_k), imprimir = False)\n",
    "    # en este punto perdemos la informacion de la iteracion anterior\n",
    "    x_k1 = x_k - alpha*grad_cuad(x_k)\n",
    "    error_ajustado = np.linalg.norm(x_k1 - x_k)/np.linalg.norm(x_k)\n",
    "    x_k = x_k1\n",
    "    print_line = [repr(N(x_i, digits=3)) for x_i in x_k]\n",
    "    #print(f\"{print_line} con {alpha=}\")\n",
    "    contador += 1\n",
    "#    pbar.update(int(1))\n",
    "#pbar.close()\n",
    "print(f\"{print_line} con {alpha=:.4f}, {contador=}, {error_ajustado=:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd086f55-297e-475b-8bbe-18e2fce1b0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -1.250000\n",
      "         Iterations: 7\n",
      "         Function evaluations: 32\n",
      "         Gradient evaluations: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.1666666195843882, 0.1666667827759691, 1.1666667974507066)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si queremos usar la interface de Optimizacion de SAGE \n",
    "np_cuad = lambda x: cuad(vector(x))\n",
    "# La funcion minimize viene en SAGE\n",
    "# la tolerancia default es 10e-6\n",
    "minimize(np_cuad, np.array(x_0), algorithm='bfgs', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c89abb-e01f-4ff1-baea-55097319de2f",
   "metadata": {},
   "source": [
    "## Algoritmo BFGS\n",
    "Es el principal algoritmo de tipo Cuasi-Newton\n",
    "\n",
    "Tiene las siguientes partes:\n",
    "$$p_k = -H_k \\nabla f_k$$\n",
    "\n",
    "Informalmente, $H_k$ se debe parecer a $(\\nabla^2 f_k)^{-1}$\n",
    "$$H_{k+1} = (Id - \\rho_k s_k y_k^T)H_k(Id - \\rho_k y_k s_k^T) + \\rho_k s_k s_k^T$$\n",
    "donde $$\\rho_k = 1/y_k^Ts_k$$ y $$s_k = x_{k+1} - x_k$$, $$y_k = \\nabla f_{k+1} - \\nabla f_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e823b89-7bc3-4c3c-9a80-3a47bbb8f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones Quasi-Newton\n",
    "def get_H_k1(f, grad_f, x_k, x_k1, H_k):\n",
    "    # Funcion para encontrar la actualizacion de H_k\n",
    "    y_k = grad_f(x_k1) - grad_f(x_k)\n",
    "    s_k = x_k1 - x_k\n",
    "    dim = len(x_k)\n",
    "    Id = identity_matrix(dim)\n",
    "    # note que sumamos 1 en denominador\n",
    "    # para mejorar la estabilidad numerica\n",
    "    # otra estrategia usada es: max(y_k*s_k, 1)\n",
    "    rho_k = 1/(y_k.dot_product(s_k) + 1)\n",
    "    skyk = rho_k*s_k.column()*y_k.row()\n",
    "    yksk = rho_k*y_k.column()*s_k.row()\n",
    "    sksk = rho_k*s_k.column()*s_k.row()\n",
    "    #print(rho_k)\n",
    "    return (Id - rho_k*skyk)*H_k*(Id - rho_k*yksk) + rho_k*sksk\n",
    "\n",
    "def theta(p_k, grad_k):\n",
    "    # Encuentra el cos(theta) donde theta es el angulo interior\n",
    "    return p_k*grad_k/p_k.norm()/grad_k.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1b41ad6-78e3-41f8-880d-9a1e12909b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sols: ['-0.167', '0.167', '1.17'] \n",
      " alpha=1.00000000000000, \n",
      " contador=32, \n",
      " error_ajustado=2.5764428439839517e-07\n"
     ]
    }
   ],
   "source": [
    "# Implementamos el metodo  Quasi-Newton\n",
    "# punto inicial para comenzar la iteracion\n",
    "x_0 = vector([2,2.5,4])\n",
    "# placeholder para x_k\n",
    "# el vector que va a llevar las iteraciones\n",
    "x_k = x_0\n",
    "# Usamos H_0 = Id\n",
    "# No es la mejor forma pero es la mas sencilla\n",
    "H_k = identity_matrix(dim)\n",
    "contador = 0\n",
    "MAX_ITER = 1000\n",
    "abs_tol = 1e-6\n",
    "error_ajustado = 1\n",
    "#pbar = tqdm(total = int(MAX_ITER + 1))\n",
    "while (contador < MAX_ITER and\n",
    "      error_ajustado > abs_tol):\n",
    "    grad_k = grad_cuad(x_k)\n",
    "    # Este es el nuevo vector de direccion de mejora\n",
    "    p_k = -H_k*grad_k\n",
    "    #p_k = -Q.inverse()*grad_k\n",
    "    alpha = backtrack(cuad, x_k, grad_k, p_k, imprimir = False)\n",
    "    \n",
    "    #print(f\"|grad|={grad_k.norm():.3f}, |p_k| = {p_k.norm():.3f} , alpha={alpha:.3f}, theta={theta(grad_k,p_k):.3f}\")\n",
    "    # encontramos la nueva iteracion\n",
    "    x_k1 = x_k + alpha*p_k\n",
    "    # actualizamos H_k\n",
    "    H_k = get_H_k1(cuad, grad_cuad, x_k, x_k1, H_k)\n",
    "    error_ajustado = np.linalg.norm(x_k1 - x_k)/np.linalg.norm(x_k)    \n",
    "    # en este punto perdemos la informacion de la iteracion anterior\n",
    "    x_k = x_k1\n",
    "    print_line = [repr(N(x_i, digits=3)) for x_i in x_k]\n",
    "    #print(f\"{print_line} con {alpha=}\")\n",
    "    contador += 1\n",
    "#    pbar.update(int(1))\n",
    "#pbar.close()\n",
    "print(f\"Sols: {print_line} \\n {alpha=}, \\n {contador=}, \\n {error_ajustado=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f396f8-b9d4-4f31-93bb-974fe586ca93",
   "metadata": {},
   "source": [
    "## Newton??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.5",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
